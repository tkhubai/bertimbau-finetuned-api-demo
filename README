# BERTimbau Fine-tuned API - Docker Implementation - Demo

## üìã Sobre o Projeto

Demo de API para infer√™ncia de modelos BERTimbau fine-tuned, containerizada com Docker e otimizada para CPU.

## üöÄ Caracter√≠sticas Principais

- **‚úÖ 100% Offline** - N√£o requer conex√£o com Hugging Face Hub
- **‚ö° Otimizado para CPU** - Configura√ß√£o espec√≠fica para melhor performance
- **üê≥ Dockerizado** - F√°cil deploy e reprodu√ß√£o de ambiente
- **üîß FastAPI** - API moderna com documenta√ß√£o autom√°tica
- **üìä Monitoramento** - Endpoints de health check e m√©tricas

## üèóÔ∏è Estrutura do Projeto

```
bertimbau-app/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api.py              # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ model.py           # Classe do modelo BERTimbau
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    # Depend√™ncias Python
‚îÇ   ‚îî‚îÄ‚îÄ test.py            # Scripts de teste
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ seu-modelo-finetuned/  # Modelo fine-tuned
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ       ‚îú‚îÄ‚îÄ vocab.txt
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ       ‚îî‚îÄ‚îÄ special_tokens_map.json
‚îú‚îÄ‚îÄ Dockerfile             # Configura√ß√£o do container
‚îî‚îÄ‚îÄ README.md             # Este arquivo
```

## üì¶ Depend√™ncias

### Python Packages (requirements.txt)
```txt
# torch==2.2.2+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html
transformers==4.45.2      # Hugging Face Transformers
protobuf==3.20.3          # Protocol Buffers
fastapi==0.115.2          # Framework API moderno
uvicorn==0.32.0           # ASGI server
pydantic==2.9.2           # Valida√ß√£o de dados
numpy==1.26.4             # Computa√ß√£o num√©rica
torch==2.2.2+cpu          # PyTorch para CPU
```

## üê≥ Configura√ß√£o Docker

### Dockerfile
```dockerfile
FROM python:3.9-slim

# Vari√°veis de ambiente para CPU
ENV CUDA_VISIBLE_DEVICES=""
ENV TF_FORCE_GPU_ALLOW_GROWTH="false"
ENV TF_CPP_MIN_LOG_LEVEL="2"

# Instalar depend√™ncias do sistema
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /workspace

# Instalar PyTorch para CPU
RUN pip install torch==2.2.2+cpu --index-url https://download.pytorch.org/whl/cpu

# Instalar demais depend√™ncias
COPY app/requirements.txt /workspace/app/requirements.txt
RUN pip install --no-cache-dir -r /workspace/app/requirements.txt

# Copiar c√≥digo e modelos
COPY app/ /workspace/app/
COPY models/ /workspace/models/

EXPOSE 8000
ENV PYTHONPATH=/workspace

CMD ["uvicorn", "app.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

## üöÄ Como Executar

### 1. Build da Imagem Docker
```bash
docker buildx build -t bertimbau-api .
```

### 2. Executar em Docker
```bash
docker run -p 8000:8000 \
  -v $(pwd)/models:/workspace/models \
  -v $(pwd)/app:/workspace/app \
  bertimbau-api
```

### 3. Executar localmente
```bash
> pip install --no-cache-dir -r requirements.txt
> PYTHONPATH=/app uvicorn app.api:app --host 0.0.0.0 --port 8000 --reload
```

## üì° Endpoints da API

### Health Check
```bash
GET http://localhost:8000/health
```
**Resposta:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "offline": true
}
```

### Informa√ß√µes do Modelo
```bash
GET http://localhost:8000/model-info
```

### Predi√ß√£o (POST)
```bash
POST http://localhost:8000/predict
Content-Type: application/json

{
  "text": "Este √© um texto de exemplo para classifica√ß√£o.",
  "max_length": 128
}
```

**Resposta:**
```json
{
  "prediction": [0],
  "probabilities": [[0.1, 0.9]],
  "model_loaded": true
}
```

## üß™ Testes

### Testar o Modelo Localmente
```bash
docker exec bertimbau-api python app/test.py
```

### Testar Configura√ß√£o CPU
```bash
docker exec bertimbau-api python app/test_cpu.py
```

### Testar via curl
```bash
# Health check
curl http://localhost:8000/health

# Predi√ß√£o
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"text":"O modelo est√° funcionando perfeitamente!"}'
```

## ‚öôÔ∏è Configura√ß√µes de Otimiza√ß√£o

### Vari√°veis de Ambiente para CPU
```bash
CUDA_VISIBLE_DEVICES=""          # Desabilita CUDA
TF_FORCE_GPU_ALLOW_GROWTH="false" # Previne aloca√ß√£o de GPU
TF_CPP_MIN_LOG_LEVEL="2"         # Reduce TensorFlow logging
PYTHONPATH="/workspace"          # Python path configuration
```

### Otimiza√ß√µes no C√≥digo
- `torch.set_grad_enabled(False)` - Desabilita gradientes
- `model.eval()` - Modo de avalia√ß√£o
- `with torch.no_grad():` - Inference sem gradientes

## üîß Troubleshooting

### Erro: Module not found
```bash
# Verificar PYTHONPATH
docker exec bertimbau-api echo $PYTHONPATH

# Verificar estrutura de arquivos
docker exec bertimbau-api ls -la /workspace/app/
```

### Erro: Modelo n√£o carrega
```bash
# Verificar se os arquivos do modelo est√£o presentes
docker exec bertimbau-api ls -la /workspace/models/seu-modelo-finetuned/
```

### Performance Lenta
```bash
# Aumentar n√∫mero de threads CPU
docker run -e OMP_NUM_THREADS=4 -e MKL_NUM_THREADS=4 ...
```

## üìä Monitoramento

### Logs da Aplica√ß√£o
```bash
docker logs bertimbau-api -f
```

### Uso de Recursos
```bash
docker stats bertimbau-api
```

### Health Check Autom√°tico
```bash
# Verificar status periodicamente
watch -n 5 curl -s http://localhost:8000/health
```

## üõ†Ô∏è Desenvolvimento

### Modo Desenvolvimento com Hot Reload
```bash
# Modificar o CMD no Dockerfile para:
CMD ["uvicorn", "app.api:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

### Testar Altera√ß√µes sem Rebuild
```bash
# O volume ./app mapeado permite altera√ß√µes em tempo real
# Edite os arquivos em ./app/ e a API recarregar√° automaticamente
```

## üìà Performance Tips

1. **Batch Processing**: Processar m√∫ltiplos textos de uma vez
2. **Tokenization Cache**: Cache de tokens para textos repetidos
3. **Model Quantization**: Usar torch.quantization para melhor performance
4. **Thread Optimization**: Ajustar OMP_NUM_THREADS baseado no CPU

## üîí Considera√ß√µes de Seguran√ßa

- ‚úÖ API roda localmente (127.0.0.1)
- ‚úÖ Sem depend√™ncias externas
- ‚úÖ Modelos locais apenas
- ‚úÖ Sem exposi√ß√£o desnecess√°ria de ports

## üìù Pr√≥ximos Passos

1. [ ] Adicionar autentica√ß√£o √† API
2. [ ] Implementar rate limiting
3. [ ] Adicionar monitoring com Prometheus
4. [ ] Criar testes unit√°rios completos
5. [ ] Implementar batch processing

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie sua feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas changes (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üÜò Suporte

Para issues e d√∫vidas:
1. Verifique os logs: `docker logs bertimbau-api`
2. Teste a configura√ß√£o: `docker exec bertimbau-api python app/test_cpu.py`
3. Verifique se todos os arquivos do modelo est√£o presentes

---

**‚≠ê Se este projeto foi √∫til, deixe uma star no reposit√≥rio!**